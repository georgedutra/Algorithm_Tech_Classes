{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1b14463-2c4d-4a53-a8d1-def39d052cff",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LAB 11 - Random Forest for Regression\n",
    "\n",
    "In this lab we will be extending the previous lab about Decision trees and build a Regression model using Random Forest.\n",
    "\n",
    "For simplicity, we will be using the same dataset as the previous lab (you can find it in ECLASS).\n",
    "\n",
    "**IMPORTANT:** For this lab, if you haven't finished your code from last week's lab on Decision trees, you will have the option to use the sklearn implementation for a regression tree. However, this doesn't mean that you should skip the previous lab. This is just so that you don't get behind with the content and you don't spend all your time today working on the previous lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3323dc4-95cd-4564-8ccd-441019188fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f29dfe0-10af-4871-ab66-e7cffe0685cc",
   "metadata": {},
   "source": [
    "As mentioned before, use the Boston Housing data and prepare your train/val/test split as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d985079",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pd.read_csv('BostonHousing.txt')\n",
    "boston.head()\n",
    "\n",
    "x, t = boston.values[:, :-1], boston.values[:, -1]\n",
    "\n",
    "x_train, x_temp, t_train, t_temp = train_test_split(x, t, train_size = 0.7)\n",
    "x_valid, x_test, t_valid, t_test = train_test_split(x_temp, t_temp, train_size = 0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b785b13",
   "metadata": {},
   "source": [
    "### Decision Tree Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "881799cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTree():\n",
    "    \"\"\" Decision Tree machine learning model \n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Creating a Decision Tree\n",
    "        \"\"\"\n",
    "        self.root = None\n",
    "        \n",
    "    def regression_criterion(self, region):\n",
    "        \"\"\"\n",
    "        Implements the sum of squared error criterion in a region\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        region : ndarray\n",
    "            Array of shape (N,) containing the target values for N datapoints in the training set.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        sse : float\n",
    "            The sum of squared error (= float(\"inf\") for empty regions)\n",
    "        \"\"\"\n",
    "        if len(region) == 0: return float(\"inf\")\n",
    "\n",
    "        y_hat = np.mean(region)\n",
    "        sse = np.sum((region-y_hat)**2)\n",
    "        return sse\n",
    "    \n",
    "    def split_region(self, region, feature_index, tau):\n",
    "        \"\"\"\n",
    "        Given a region, splits it in two based on the passed feature, using tau as threshold\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        region : array of size (n_samples, n_features)\n",
    "            a partition of the dataset (or the full dataset) to be split\n",
    "        feature_index : int\n",
    "            the index of the feature (column of the region array) used to make this partition\n",
    "        tau : float\n",
    "            The threshold used to make the partitions\n",
    "            \n",
    "        Return\n",
    "        ------\n",
    "        left_partition : array\n",
    "            indices of the datapoints in `region` where feature < `tau`\n",
    "        right_partition : array\n",
    "            indices of the datapoints in `region` where feature >= `tau` \n",
    "        \"\"\"\n",
    "        mask = region[:, feature_index] < tau # here I create a mask that says True for a line where the \"feature_index\" column is lower than \"tau\"\n",
    "\n",
    "        left_partition = np.nonzero(mask)[0]\n",
    "        right_partition = np.nonzero(~mask)[0] # Using the logical inverse of the mask\n",
    "        return left_partition, right_partition\n",
    "\n",
    "    def get_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Given a dataset (full or partial), splits it on the feature of that minimizes the sum of squared error\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array (n_samples, n_features)\n",
    "            features \n",
    "        y : array (n_samples, )\n",
    "            target labels\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        decision : dictionary\n",
    "            keys:\n",
    "            * 'feature_index' -> an integer that indicates the feature (column) of `X` on which the data is split\n",
    "            * 'tau' -> the threshold used to make the split\n",
    "            * 'left_region' -> array of indices where the `feature_index`th feature of X is lower than `tau`\n",
    "            * 'right_region' -> indices not in `left_region`\n",
    "        \"\"\"\n",
    "        best_error = float(\"inf\")\n",
    "\n",
    "        for col in range(X.shape[1]):\n",
    "            for datapoint in range(X.shape[0]):\n",
    "                tau = X[datapoint, col]\n",
    "                l, r = self.split_region(X, col, tau)\n",
    "                \n",
    "                l_error = self.regression_criterion(y[l])\n",
    "                r_error = self.regression_criterion(y[r])\n",
    "                error = l_error + r_error\n",
    "\n",
    "                if error < best_error:\n",
    "                    best_error = error\n",
    "                    decision = {\"feature_index\": col, \"tau\": tau, \"left_region\": l, \"right_region\": r}\n",
    "\n",
    "        return decision\n",
    "\n",
    "    def recursive_growth(self, min_samples, max_depth, error_criterion, X, y, node = None, current_depth = 0):\n",
    "        \"\"\"\n",
    "        Recursively grows itself by optimizing the error in each node and obeying the passed stopping criterions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        min_samples : int\n",
    "            parameter for stopping criterion if a node has <= min_samples datapoints\n",
    "        max_depth : int\n",
    "            parameter for stopping criterion if a node belongs to this depth\n",
    "        error_criterion : float\n",
    "            error value to act as stop criterion\n",
    "\n",
    "        X : array (n_samples, n_features)\n",
    "            features (full dataset)\n",
    "        y : array (n_samples, )\n",
    "            labels (full dataset)\n",
    "        \n",
    "        node : dictionary, defaults to self.root\n",
    "            When passed, the recursive growth will start in this node, and not in the root.\n",
    "            If the passed node is terminal, it contains only the \"value\" key, which determines the value to be used as a prediction.\n",
    "            Else, the dictionary has the structure defined by `get_split`.\n",
    "            \n",
    "        current_depth : int, defaults to 0\n",
    "            current distance from the root\n",
    "        \"\"\"\n",
    "        if not node:\n",
    "            self.root = self.get_split(X, y)\n",
    "            node = self.root\n",
    "\n",
    "        if \"left_region\" in node:\n",
    "            l = node[\"left_region\"]\n",
    "            r = node[\"right_region\"]\n",
    "            \n",
    "            # Process left\n",
    "            if (y[l].size <= min_samples) or (current_depth == max_depth) or (self.regression_criterion(y[l]) <= error_criterion):\n",
    "                node[\"left\"] = {\"value\": np.mean(y[l])}\n",
    "            else:\n",
    "                node[\"left\"] = self.get_split(X[l], y[l])\n",
    "                self.recursive_growth(min_samples, max_depth, error_criterion, X, y, node[\"left\"], current_depth+1)   \n",
    "            \n",
    "            # Process right\n",
    "            if (y[r].size <= min_samples) or (current_depth == max_depth) or (self.regression_criterion(y[r]) <= error_criterion):\n",
    "                node[\"right\"] = {\"value\": np.mean(y[r])}\n",
    "            else:\n",
    "                node[\"right\"] = self.get_split(X[r], y[r])\n",
    "                self.recursive_growth(min_samples, max_depth, error_criterion, X, y, node[\"right\"], current_depth+1)\n",
    "\n",
    "    def print_tree(self, node = None, depth = 0):\n",
    "        \"\"\"Prints the whole tree\n",
    "\n",
    "        :param node: Node from where to start printing, defaults to self.root\n",
    "        :type node: dict, optional\n",
    "        :param depth: Current depth of the node, defaults to 0 (root depth)\n",
    "        :type depth: int, optional\n",
    "        \"\"\"\n",
    "        if not node: \n",
    "            node = self.root\n",
    "\n",
    "        if 'value' in node.keys():\n",
    "            print('.  '*(depth-1), f\"[{node['value']}]\")\n",
    "        else:\n",
    "            print('.  '*depth, f'X_{node[\"feature_index\"]} < {node[\"tau\"]}')\n",
    "            self.print_tree(node['left'], depth+1)\n",
    "            self.print_tree(node['right'], depth+1)\n",
    "\n",
    "    def __predict_sample(self, sample, node = None):\n",
    "        \"\"\"\n",
    "        Makes a prediction based on the decision tree defined by `node`\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        sample : array of size (n_features,)\n",
    "            a sample datapoint to be predicted\n",
    "        node : dict, defaults to self.root\n",
    "            A node created by one of the methods above\n",
    "        \"\"\"\n",
    "        if not node: \n",
    "            node = self.root\n",
    "\n",
    "        if \"value\" in node.keys():\n",
    "            return node[\"value\"]\n",
    "        \n",
    "        if sample[node[\"feature_index\"]] < node[\"tau\"]:\n",
    "            return self.__predict_sample(sample, node[\"left\"])\n",
    "        \n",
    "        return self.__predict_sample(sample, node[\"right\"])\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Makes a prediction based on the fitted training data\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array of size (n_samples, n_features)\n",
    "            n_samples predictions will be made\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray (n_samples, )\n",
    "            Unidimensional array with predicted values or labels for each datapoint in the dataset 'X'\n",
    "        \"\"\"\n",
    "        prediction = []\n",
    "        for datapoint in X:\n",
    "            prediction.append(self.__predict_sample(datapoint))\n",
    "        \n",
    "        return np.array(prediction)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7de6c793-4f97-49ff-a1f3-3e5ec2aa086b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 1 -- Bootstrap\n",
    "\n",
    "Also known as [bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating), this technique consists of making several samples with replacement of the original data, using each of the samples to train an estimator, and then aggregating the predictions using the average (this is also a type of model ensemble)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7aaba32-4c00-404c-9899-7a5759059598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(X, num_bags=10):\n",
    "    \"\"\"\n",
    "    Given a dataset and a number of bags,\n",
    "    sample the dataset with replacement.\n",
    "    \n",
    "    This function does not return a copy\n",
    "    of the datapoints, but a list of indices\n",
    "    with compatible dimensionality\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray\n",
    "        A dataset\n",
    "    num_bags : int, default 10\n",
    "        The number of bags to create\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of ndarray\n",
    "        The list contains `num_bags` integer one-dimensional ndarrays.\n",
    "        Each of these contains the indices corresponding to the \n",
    "        sampled datapoints in `X`\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    * The number of datapoints in each bach will\n",
    "      match the number of datapoints in the given\n",
    "      dataset.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(0) # you can change the seed, or use 0 to replicate my results\n",
    "    return [rng.integers(0, X.shape[0], X.shape[0]) for i in range(num_bags)] \n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff32d32b-f615-43da-b70e-aa8959d01093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([85, 63, 51, 26, 30,  4,  7,  1, 17, 81, 64, 91, 50, 60, 97, 72, 63,\n",
       "       54, 55, 93, 27, 81, 67,  0, 39, 85, 55,  3, 76, 72, 84, 17,  8, 86,\n",
       "        2, 54,  8, 29, 48, 42, 40,  2,  0, 12,  0, 67, 52, 64, 25, 61, 76,\n",
       "       38, 46, 99, 80, 98, 37, 68, 95, 65, 84, 68, 70, 38, 87, 13, 57, 72,\n",
       "       84, 52, 37, 31, 42, 48, 71, 88,  7, 93, 53, 35, 67, 57, 25, 32, 71,\n",
       "       59, 50, 33, 76, 39, 32, 89, 26, 22, 71, 62,  4,  8, 37, 83],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "X_small = rng.random(size=(100,2))\n",
    "bags = bootstrap(X_small)\n",
    "bags[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f32b5126-5aad-4c74-a7de-ad3935acd7e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 2 -- Aggregation\n",
    "\n",
    "The second part of bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baac85e0-2a36-4ee0-92a5-e31a01a70928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_regression(preds):\n",
    "    \"\"\"\n",
    "    Aggregate predictions by several estimators\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    preds : list of ndarray\n",
    "        Predictions from multiple estimators.\n",
    "        All ndarrays in this list should have the same\n",
    "        dimensionality.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    ndarray\n",
    "        The mean of the predictions\n",
    "    \"\"\"\n",
    "    return np.mean(np.array(preds), 0)\n",
    "    # Your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b91f57a-6395-4285-ac01-e1099af84dde",
   "metadata": {},
   "source": [
    "## Exercise 3 -- Random Forest for regression\n",
    "\n",
    "Using the functions you implemented above, it is now time to put all of them together to train several decision trees and then ensemble them to output a single prediction. For the random forest, however, we need to select a subset of features at each split on the decision tree. \n",
    "\n",
    "For this part, you can use the sklearn implementation of Random forest for regression as your estimator for each set of features and bags. See below an example of how to do this, and always remember to check the necessary documentation when using an external function.\n",
    "\n",
    "Some parameters you will have to set are: \n",
    "* num_features: number of features per estimator\n",
    "* min_samples: min number of samples per leaf node\n",
    "* max_depth: maximum depth of the decision tree (each estimator)\n",
    "* num_estimators: number of decision trees you will create using each bag and random set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d454908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_set(feat_num, chosen_num):\n",
    "    \"\"\"Chooses a subset of features between the total number\n",
    "\n",
    "    :param feat_num: Original number of features\n",
    "    :type feat_num: int\n",
    "    :param chosen_num: Number of features to be chosen between the original ones\n",
    "    :type chosen_num: int\n",
    "\n",
    "    :return: List of indexes to access a subset of the features\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    chosen_feats = random.choices(range(feat_num), k = chosen_num)\n",
    "    return chosen_feats\n",
    "\n",
    "def fit_bagging(X, y, num_features, num_estimators, min_samples, max_depht):\n",
    "    \"\"\"Creates a random forest model with 'num_estimators' trees, each trained with a subset of 'num_features' features between the original ones in the dataset 'X'\n",
    "\n",
    "    :param X: Full dataset to fit in the random forest's trees\n",
    "    :type X: np.ndarray\n",
    "    :param y: Full dataset labels\n",
    "    :type y: np.ndarray\n",
    "\n",
    "    :param num_features: Number of features to use in each singular tree\n",
    "    :type num_features: int\n",
    "    :param num_estimators: Number of trees in the random forest\n",
    "    :type num_estimators: int\n",
    "    :param min_samples: Parameter for a tree's stopping criterion if a node has <= min_samples datapoints\n",
    "    :type min_samples: int\n",
    "    :param max_depht: Parameter for a tree's stopping criterion if a node belongs to this depth\n",
    "    :type max_depht: int\n",
    "\n",
    "    :return: Returns a list containing 'num_estimators' objects from the DTree() class, trained through bagging process\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    bags = bootstrap(X, num_estimators)\n",
    "    forest = []\n",
    "\n",
    "    for i in range(num_estimators):\n",
    "        estimator = DTree()\n",
    "\n",
    "        # Separating a subset with fewer features and the same amount of datapoints (random with replacement)\n",
    "        features = get_features_set(n_features, num_features)\n",
    "        X_subset = X[bags[i]][:, features]\n",
    "        y_subset = y[bags[i]]\n",
    "        \n",
    "        # Training the tree with the defined subsets of data\n",
    "        estimator.recursive_growth(min_samples, max_depht, 0, X_subset, y_subset)\n",
    "\n",
    "        forest.append(estimator)\n",
    "        \n",
    "    return forest\n",
    "\n",
    "def predict(estimators: list[DTree], X):\n",
    "    \"\"\"Makes the prediction for a Random Forest Model\n",
    "\n",
    "    :param estimators: List with many DTree class estimators\n",
    "    :type estimators: list[DTree]\n",
    "    :param X: Dataset to predict the labels\n",
    "    :type X: np.ndarray\n",
    "    :return: Returns an array with the average from all the predictions in the random forest\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "\n",
    "    for tree in estimators:\n",
    "        predictions.append(tree.predict(X))\n",
    "    \n",
    "    return aggregate_regression(predictions)\n",
    "\n",
    "def RMSE(predict, target):\n",
    "    return np.sqrt(np.mean((predict - target)**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48ecbd9d-9628-42d4-88ee-c3c3ea408e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE = 8.80501069845815\n"
     ]
    }
   ],
   "source": [
    "## your code goes here:\n",
    "num_features = 4 # The slides say the common value is approx sqrt(n_features)\n",
    "num_estimators = 50\n",
    "min_samples = 25\n",
    "max_depht = 5\n",
    "\n",
    "random_forest = fit_bagging(x_train, t_train, num_features, num_estimators, min_samples, max_depht)\n",
    "y_pred = predict(random_forest, x_valid)\n",
    "\n",
    "print(\"Validation RMSE =\", RMSE(y_pred, t_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
